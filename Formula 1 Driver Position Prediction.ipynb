{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c274a7e8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-28T20:12:44.408137Z",
     "iopub.status.busy": "2024-06-28T20:12:44.407809Z",
     "iopub.status.idle": "2024-06-28T20:30:21.513842Z",
     "shell.execute_reply": "2024-06-28T20:30:21.512934Z"
    },
    "papermill": {
     "duration": 1057.112491,
     "end_time": "2024-06-28T20:30:21.516342",
     "exception": false,
     "start_time": "2024-06-28T20:12:44.403851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/1894789148.py:37: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  train_data.fillna(method='ffill', inplace=True)\n",
      "/tmp/ipykernel_23/1894789148.py:38: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  train_data.fillna(method='bfill', inplace=True)\n",
      "/tmp/ipykernel_23/1894789148.py:39: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  val_data.fillna(method='ffill', inplace=True)\n",
      "/tmp/ipykernel_23/1894789148.py:40: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  val_data.fillna(method='bfill', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.698918 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1428\n",
      "[LightGBM] [Info] Number of data points in the train set: 1886734, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 11.105223\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END colsample_bytree=0.749816047538945, learning_rate=0.10507143064099161, max_depth=5, n_estimators=121, subsample=0.8394633936788146; total time=  46.3s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.679822 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1428\n",
      "[LightGBM] [Info] Number of data points in the train set: 1886734, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 11.105223\n",
      "[CV] END colsample_bytree=0.6571467271687763, learning_rate=0.07508884729488528, max_depth=3, n_estimators=51, subsample=0.8887995089067299; total time=  15.4s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.507251 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1334\n",
      "[LightGBM] [Info] Number of data points in the train set: 1886734, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 13.859754\n",
      "[CV] END colsample_bytree=0.6571467271687763, learning_rate=0.07508884729488528, max_depth=3, n_estimators=51, subsample=0.8887995089067299; total time=  19.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.680992 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1428\n",
      "[LightGBM] [Info] Number of data points in the train set: 1886734, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 11.105223\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END colsample_bytree=0.6624074561769746, learning_rate=0.025599452033620268, max_depth=5, n_estimators=137, subsample=0.7334834444556088; total time=  51.9s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.691539 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1701\n",
      "[LightGBM] [Info] Number of data points in the train set: 1886734, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 12.031026\n",
      "[CV] END colsample_bytree=0.6571467271687763, learning_rate=0.07508884729488528, max_depth=3, n_estimators=51, subsample=0.8887995089067299; total time=  16.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.663281 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1428\n",
      "[LightGBM] [Info] Number of data points in the train set: 1886734, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 11.105223\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END colsample_bytree=0.9754210836063001, learning_rate=0.010077876584101433, max_depth=6, n_estimators=70, subsample=0.8469926038510867; total time=  42.4s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.999338 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1701\n",
      "[LightGBM] [Info] Number of data points in the train set: 1886734, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 12.031026\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END colsample_bytree=0.749816047538945, learning_rate=0.10507143064099161, max_depth=5, n_estimators=121, subsample=0.8394633936788146; total time=  43.3s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.621824 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1701\n",
      "[LightGBM] [Info] Number of data points in the train set: 1886734, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 12.031026\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END colsample_bytree=0.6624074561769746, learning_rate=0.025599452033620268, max_depth=5, n_estimators=137, subsample=0.7334834444556088; total time=  47.4s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.598306 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1334\n",
      "[LightGBM] [Info] Number of data points in the train set: 1886734, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 13.859754\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END colsample_bytree=0.9754210836063001, learning_rate=0.010077876584101433, max_depth=6, n_estimators=70, subsample=0.8469926038510867; total time=  32.2s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.560387 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1334\n",
      "[LightGBM] [Info] Number of data points in the train set: 1886734, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 13.859754\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END colsample_bytree=0.749816047538945, learning_rate=0.10507143064099161, max_depth=5, n_estimators=121, subsample=0.8394633936788146; total time=  44.4s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.572753 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1334\n",
      "[LightGBM] [Info] Number of data points in the train set: 1886734, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 13.859754\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END colsample_bytree=0.6624074561769746, learning_rate=0.025599452033620268, max_depth=5, n_estimators=137, subsample=0.7334834444556088; total time=  47.0s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.695103 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1428\n",
      "[LightGBM] [Info] Number of data points in the train set: 1886734, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 11.105223\n",
      "[CV] END colsample_bytree=0.8446612641953124, learning_rate=0.01070663052197174, max_depth=3, n_estimators=98, subsample=0.8099098641033556; total time=  32.2s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.665746 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1701\n",
      "[LightGBM] [Info] Number of data points in the train set: 1886734, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 12.031026\n",
      "[CV] END colsample_bytree=0.8446612641953124, learning_rate=0.01070663052197174, max_depth=3, n_estimators=98, subsample=0.8099098641033556; total time=  28.7s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.693670 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1334\n",
      "[LightGBM] [Info] Number of data points in the train set: 1886734, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 13.859754\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END colsample_bytree=0.7599443886861021, learning_rate=0.014666566321361544, max_depth=6, n_estimators=64, subsample=0.7824279936868144; total time=  27.6s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.656770 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1428\n",
      "[LightGBM] [Info] Number of data points in the train set: 1886734, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 11.105223\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END colsample_bytree=0.9439761626945282, learning_rate=0.07803075385877797, max_depth=3, n_estimators=67, subsample=0.9795542149013333; total time=  21.1s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.714191 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1701\n",
      "[LightGBM] [Info] Number of data points in the train set: 1886734, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 12.031026\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END colsample_bytree=0.9754210836063001, learning_rate=0.010077876584101433, max_depth=6, n_estimators=70, subsample=0.8469926038510867; total time=  39.7s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.010735 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1701\n",
      "[LightGBM] [Info] Number of data points in the train set: 1886734, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 12.031026\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END colsample_bytree=0.7599443886861021, learning_rate=0.014666566321361544, max_depth=6, n_estimators=64, subsample=0.7824279936868144; total time=  31.7s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.863537 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1701\n",
      "[LightGBM] [Info] Number of data points in the train set: 1886734, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 12.031026\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END colsample_bytree=0.9140703845572055, learning_rate=0.029967378215835977, max_depth=5, n_estimators=113, subsample=0.786705157299192; total time=  56.9s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.853641 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1428\n",
      "[LightGBM] [Info] Number of data points in the train set: 1886734, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 11.105223\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END colsample_bytree=0.7599443886861021, learning_rate=0.014666566321361544, max_depth=6, n_estimators=64, subsample=0.7824279936868144; total time=  34.0s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.553465 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1334\n",
      "[LightGBM] [Info] Number of data points in the train set: 1886734, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 13.859754\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END colsample_bytree=0.9140703845572055, learning_rate=0.029967378215835977, max_depth=5, n_estimators=113, subsample=0.786705157299192; total time=  55.0s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.647038 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1428\n",
      "[LightGBM] [Info] Number of data points in the train set: 1886734, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 11.105223\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END colsample_bytree=0.9862528132298237, learning_rate=0.09083973481164612, max_depth=3, n_estimators=139, subsample=0.6390688456025535; total time=  44.8s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.961576 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1334\n",
      "[LightGBM] [Info] Number of data points in the train set: 1886734, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 13.859754\n",
      "[CV] END colsample_bytree=0.8446612641953124, learning_rate=0.01070663052197174, max_depth=3, n_estimators=98, subsample=0.8099098641033556; total time=  26.0s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.684328 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1428\n",
      "[LightGBM] [Info] Number of data points in the train set: 1886734, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 11.105223\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END colsample_bytree=0.9140703845572055, learning_rate=0.029967378215835977, max_depth=5, n_estimators=113, subsample=0.786705157299192; total time= 1.1min\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.723647 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1701\n",
      "[LightGBM] [Info] Number of data points in the train set: 1886734, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 12.031026\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END colsample_bytree=0.9862528132298237, learning_rate=0.09083973481164612, max_depth=3, n_estimators=139, subsample=0.6390688456025535; total time=  45.6s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.632137 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1701\n",
      "[LightGBM] [Info] Number of data points in the train set: 1886734, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 12.031026\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END colsample_bytree=0.9439761626945282, learning_rate=0.07803075385877797, max_depth=3, n_estimators=67, subsample=0.9795542149013333; total time=  24.5s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.814174 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1334\n",
      "[LightGBM] [Info] Number of data points in the train set: 1886734, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 13.859754\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END colsample_bytree=0.9862528132298237, learning_rate=0.09083973481164612, max_depth=3, n_estimators=139, subsample=0.6390688456025535; total time=  34.7s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.626480 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1701\n",
      "[LightGBM] [Info] Number of data points in the train set: 1886734, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 12.031026\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[CV] END colsample_bytree=0.8736932106048627, learning_rate=0.054015249373960135, max_depth=5, n_estimators=57, subsample=0.6137554084460873; total time=  19.9s\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.435875 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1724\n",
      "[LightGBM] [Info] Number of data points in the train set: 2830101, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 12.332001\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/1894789148.py:104: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_data.fillna(method='ffill', inplace=True)\n",
      "/tmp/ipykernel_23/1894789148.py:105: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_data.fillna(method='bfill', inplace=True)\n",
      "/tmp/ipykernel_23/1894789148.py:107: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  test_data['age'] = pd.to_datetime(test_data['date']).dt.year - pd.to_datetime(test_data['dob']).dt.year\n",
      "/tmp/ipykernel_23/1894789148.py:107: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  test_data['age'] = pd.to_datetime(test_data['date']).dt.year - pd.to_datetime(test_data['dob']).dt.year\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from lightgbm import LGBMRegressor\n",
    "from scipy.stats import randint, uniform\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Load data\n",
    "logger.info('Loading training and validation data')\n",
    "train_data = pd.read_csv('/kaggle/input/f1nalyze-datathon-ieeecsmuj/train.csv', low_memory=False)\n",
    "val_data = pd.read_csv('/kaggle/input/f1nalyze-datathon-ieeecsmuj/validation.csv', low_memory=False)\n",
    "test_data = pd.read_csv('/kaggle/input/f1nalyze-datathon-ieeecsmuj/test.csv', low_memory=False)\n",
    "\n",
    "# Define mixed type columns\n",
    "dtype_dict = {'number': str, 'positionText_x': str, 'time_x': str, 'fastestLapTime': str, 'status': str}\n",
    "\n",
    "# Reload data with specified dtype\n",
    "logger.info('Reloading data with specified dtypes')\n",
    "train_data = pd.read_csv('/kaggle/input/f1nalyze-datathon-ieeecsmuj/train.csv', dtype=dtype_dict, low_memory=False)\n",
    "val_data = pd.read_csv('/kaggle/input/f1nalyze-datathon-ieeecsmuj/validation.csv', dtype=dtype_dict, low_memory=False)\n",
    "\n",
    "# Convert to numeric and fill missing values\n",
    "logger.info('Converting to numeric and filling missing values')\n",
    "numeric_columns = ['points', 'laps', 'timetaken_in_millisec', 'fastestLap', 'max_speed', 'rank', 'grid', 'positionOrder']\n",
    "for col in numeric_columns:\n",
    "    train_data[col] = pd.to_numeric(train_data[col], errors='coerce')\n",
    "    val_data[col] = pd.to_numeric(val_data[col], errors='coerce')\n",
    "\n",
    "train_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "val_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "train_data.fillna(method='ffill', inplace=True)\n",
    "train_data.fillna(method='bfill', inplace=True)\n",
    "val_data.fillna(method='ffill', inplace=True)\n",
    "val_data.fillna(method='bfill', inplace=True)\n",
    "\n",
    "# Feature engineering\n",
    "logger.info('Performing feature engineering')\n",
    "train_data['age'] = pd.to_datetime(train_data['date']).dt.year - pd.to_datetime(train_data['dob']).dt.year\n",
    "val_data['age'] = pd.to_datetime(val_data['date']).dt.year - pd.to_datetime(val_data['dob']).dt.year\n",
    "\n",
    "train_data['avg_speed'] = train_data['timetaken_in_millisec'] / train_data['laps']\n",
    "val_data['avg_speed'] = val_data['timetaken_in_millisec'] / val_data['laps']\n",
    "\n",
    "train_data['start_grid_diff'] = train_data['grid'] - train_data['positionOrder']\n",
    "val_data['start_grid_diff'] = val_data['grid'] - val_data['positionOrder']\n",
    "\n",
    "# Label encoding for categorical variables\n",
    "logger.info('Label encoding categorical variables')\n",
    "label_encoders = {}\n",
    "categorical_columns = ['driverRef', 'constructorRef']\n",
    "\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train_data[col])  # Fit on training data\n",
    "    train_data[col] = le.transform(train_data[col])\n",
    "    val_data[col] = val_data[col].map(lambda s: le.transform([s])[0] if s in le.classes_ else -1)\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Define features\n",
    "features = [\n",
    "    'grid', 'points', 'laps', 'timetaken_in_millisec', 'fastestLap', 'max_speed', \n",
    "    'age', 'avg_speed', 'start_grid_diff', 'rank', 'year', 'round', 'circuitId', \n",
    "    'driverRef', 'constructorRef'\n",
    "]\n",
    "\n",
    "# Prepare training and validation sets\n",
    "logger.info('Preparing training and validation sets')\n",
    "X_train = train_data[features]\n",
    "y_train = train_data['position']\n",
    "X_val = val_data[features]\n",
    "y_val = val_data['position']\n",
    "\n",
    "# Model tuning with RandomizedSearchCV for LightGBM\n",
    "logger.info('Starting RandomizedSearchCV for LightGBM')\n",
    "model = LGBMRegressor(random_state=42)\n",
    "param_dist = {\n",
    "    'n_estimators': randint(50, 200),\n",
    "    'max_depth': randint(3, 7),\n",
    "    'learning_rate': uniform(0.01, 0.1),\n",
    "    'subsample': uniform(0.6, 0.4),\n",
    "    'colsample_bytree': uniform(0.6, 0.4)\n",
    "}\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, cv=3, scoring='neg_mean_squared_error', verbose=2, n_jobs=-1, random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model and prediction\n",
    "logger.info('Best model and prediction')\n",
    "best_model = random_search.best_estimator_\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
    "logger.info(f'Validation RMSE: {rmse}')\n",
    "\n",
    "# Test data preparation and prediction\n",
    "logger.info('Preparing test data and making predictions')\n",
    "for col in numeric_columns:\n",
    "    test_data[col] = pd.to_numeric(test_data[col], errors='coerce')\n",
    "\n",
    "test_data.fillna(method='ffill', inplace=True)\n",
    "test_data.fillna(method='bfill', inplace=True)\n",
    "\n",
    "test_data['age'] = pd.to_datetime(test_data['date']).dt.year - pd.to_datetime(test_data['dob']).dt.year\n",
    "test_data['avg_speed'] = test_data['timetaken_in_millisec'] / test_data['laps']\n",
    "test_data['start_grid_diff'] = test_data['grid'] - test_data['positionOrder']\n",
    "\n",
    "for col in categorical_columns:\n",
    "    test_data[col] = test_data[col].map(lambda s: label_encoders[col].transform([s])[0] if s in label_encoders[col].classes_ else -1)\n",
    "\n",
    "X_test = test_data[features]\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "# Submission\n",
    "logger.info('Creating submission file')\n",
    "submission = pd.DataFrame({'result_driver_standing': test_data['result_driver_standing'], 'position': y_test_pred})\n",
    "submission.to_csv('submission_lgbm.csv', index=False)\n",
    "logger.info('Submission file created')\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8965849,
     "sourceId": 82253,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1062.604207,
   "end_time": "2024-06-28T20:30:24.143698",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-28T20:12:41.539491",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
